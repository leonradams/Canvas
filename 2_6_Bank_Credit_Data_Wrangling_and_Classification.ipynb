{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "2.6_Bank_Credit_Data_Wrangling_and_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonradams/Canvas/blob/main/2_6_Bank_Credit_Data_Wrangling_and_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfmfqwuApc2h"
      },
      "source": [
        "# This assignment provides hands-on data wrangling and classification experience\n",
        "For this assignment, you are a Machine Learning Engineer at a local bank, and your task is to analyze whether loan applicants qualify, based on their personal information.\n",
        "Tasks are:\n",
        "I. Data wrangling to perform one-hot-encoding for the data and normalization\n",
        "II. Eliminating unimportant features\n",
        "III. Performing classification to predict qualification for loan in future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWibA4lcO-P-"
      },
      "source": [
        "### Import the necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJf_0Ce3pc2i"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "#from tensorflow.contrib.tensorboard.plugins import projector\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZI-Envb6c4J"
      },
      "source": [
        "### Load the training data. Print out its shape and first few rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5njoB1HRpc2o"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/FourthBrain/2.-Classification/main/bank-full.csv?token=ARJV76TWTYENKFZKXB4CUYS7SIDOM\"\n",
        "df_train = pd.read_csv(url, sep=';')\n",
        "print(np.shape(df_train))\n",
        "df_train.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEzNSJaWzuYr"
      },
      "source": [
        "### Exercise: Perform Exploratory Data Analysis on each feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGYERv-Jpc2r"
      },
      "source": [
        "### START CODE HERE ###\n",
        "# Get the info (column number and name, non-null count, and dtype) for each column in the training df\n",
        "None\n",
        "# Get summary statistics for the training df with the describe() method\n",
        "None\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9586xIUz-ji"
      },
      "source": [
        "### Exercise: Visualize the distributions of each numeric feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev8UvLP3jgDZ"
      },
      "source": [
        "### START CODE HERE ###\n",
        "# Use a list comprehension to extract the names of the numeric columns of the training df \n",
        "# (those with an integer dtype)\n",
        "numeric_columns = None\n",
        "# Loop through the numeric columns\n",
        "for col in None:\n",
        "    # Generate a histogram of the current numeric feature column\n",
        "    plt.hist(None)\n",
        "    plt.title(f'{col} distribution')\n",
        "    plt.show()\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wibVprU0wdX"
      },
      "source": [
        "### Exercise: Inspect the unique values of the non-numeric columns, and their counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ko1zBgNabH7"
      },
      "source": [
        "### START CODE HERE ###\n",
        "# Use a list comprehension to list the names of the non-numeric columns in the training df\n",
        "# (those with an object dtype)\n",
        "non_numeric_columns = None\n",
        "# Loop through the non-numeric columns\n",
        "for col in None:\n",
        "    # Print the counts of each unique value in the current column\n",
        "    print(None)\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OHtVFr20_-l"
      },
      "source": [
        "### Exercise: Some columns are 'yes'/'no' binaries. Map them to integer binaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ik4bT9n1LEO"
      },
      "source": [
        "### START CODE HERE ###\n",
        "# Make a list of the binary columns. There should be 4 of them. \n",
        "# Find their names in the output of the previous exercise.\n",
        "binary_columns = None\n",
        "# Create a dictionary in which the keys map to the values\n",
        "# Remember, we want to replace 'no' and 'yes' with the appropriate integers\n",
        "binary_mapping = None\n",
        "# Map the binary columns\n",
        "for col in None:\n",
        "    # Apply the mapping to the current column with the map() method for Pandas Series\n",
        "    df_train[col] = None\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tZbFZRy1TdY"
      },
      "source": [
        "### Exericse: Map the months from strings to integers, with 1 corresponding to January, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jkMpHT-1f0d"
      },
      "source": [
        "### START CODE HERE ###\n",
        "# Create the mapping dictionary\n",
        "# Make sure the months' names are formatted properly\n",
        "month_mapping = None\n",
        "# Map the months\n",
        "df_train['month'] = None\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCRK5NZY1v7A"
      },
      "source": [
        "### Exercise: One-hot encode the remaining categorical features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwyM92cd14Wj"
      },
      "source": [
        "### START CODE HERE ###\n",
        "# List the categorical features. There should be 5 of them.\n",
        "categorical_columns = None\n",
        "# One-hot encode the categorical features with the get_dummies() function for Pandas DataFrames\n",
        "df_train = None\n",
        "### E3ND CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0pc6xNN2Cul"
      },
      "source": [
        "### Inspect the first few rows of our preprocessed DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7mDSqKzpc24"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq4c7TFQ2OR4"
      },
      "source": [
        "### Exercise: Extract the target values from the training DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrKoG2XamTx6"
      },
      "source": [
        "### START CODE HERE ###\n",
        "# Extract the target values from the training DataFrame\n",
        "y = None\n",
        "# Drop the target column from the training DataFrame\n",
        "df_train = None\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10g_mzUD2SpY"
      },
      "source": [
        "### Exercise: Inspect how balanced y is\n",
        "\n",
        "For a balanced binary target, approximately half of its entries will be 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "439nSn-knDqq"
      },
      "source": [
        "### START CODE HERE ###\n",
        "# Compute the fraction of positive values in the target\n",
        "None\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z6J4ulPAat8"
      },
      "source": [
        "### Print the current number of features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zIQNf8sn-ej"
      },
      "source": [
        "print(f'The training df currently has {df_train.shape[1]} features')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkwNHtlLAwLa"
      },
      "source": [
        "### Exercise: Visually inspect the Pearson Correlations of each feature\n",
        "\n",
        "Pandas provides the `corr()` method for DataFrames for this purpose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fivN56tpc3C"
      },
      "source": [
        "plt.figure(figsize=(12,10))\n",
        "### START CODE HERE ###\n",
        "# Compute the Pearson Correlation of each feature in the training df\n",
        "cor = None\n",
        "### END CODE HERE ###\n",
        "# Plot the correlations\n",
        "sns.heatmap(cor)\n",
        "plt.show()\n",
        "# Some intersting correlations pop up that need further analysis (later)\n",
        "# we need to eliminate features that have very high absolute correlations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZKtNfIPE6RH"
      },
      "source": [
        "## Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2nx55ZHBhVf"
      },
      "source": [
        "### Exercise: Drop feature columns which have a high absolute Pearson Correlation\n",
        "\n",
        "Let's choose to drop the higher-indexed of the two correlated columns, i.e. the one that appears farther to the right in the df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf1nQfsUpc3F"
      },
      "source": [
        "### START CODE HERE ### \n",
        "# Initialize a list indicating whether to keep a column in the training df\n",
        "# For now, set all entries to True\n",
        "keep_columns = None\n",
        "# Loop over all columns\n",
        "for i in None:\n",
        "    # Loop over all columns to the right of the current one\n",
        "    for j in None:\n",
        "        # If the absolute correlation between the current two columns is greater than or equal to 0.8...\n",
        "        if None >= 0.8:\n",
        "            # If we haven't already told the keep_columns list to drop column j...\n",
        "            if None:\n",
        "                # Drop column j\n",
        "                None\n",
        "# Extract the columns to keep from the training df\n",
        "selected_columns = None\n",
        "# Make a new df with the columns we've decided to keep from the training df\n",
        "df_out = None\n",
        "### END CODE HERE ###\n",
        "print('The following columns are present in the new df:')\n",
        "print(selected_columns)\n",
        "print(f'The old df had {df_train.shape[1]} features. The new df has {df_out.shape[1]} features.')\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53PikdK0FMIm"
      },
      "source": [
        "### Feature Selection on p-value (statistical significance to Y value)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4f9H6mOpc3H"
      },
      "source": [
        "selected_columns = selected_columns[0:]\n",
        "import statsmodels.api as sm\n",
        "def backwardElimination(x, y, sl, columns):\n",
        "    numVars = len(x[0])\n",
        "    for i in range(0, numVars):\n",
        "        regressor_OLS = sm.OLS(y, x).fit()\n",
        "        #maxVar = max(regressor_OLS.pvalues).astype(float)\n",
        "        maxVar = max(regressor_OLS.pvalues)\n",
        "        if maxVar > sl:\n",
        "            for j in range(0, numVars - i):\n",
        "                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
        "                    x = np.delete(x, j, 1)\n",
        "                    columns = np.delete(columns, j)\n",
        "                    \n",
        "    regressor_OLS.summary()\n",
        "    return x, columns\n",
        "SL = 0.01\n",
        "# Note that we're passing in df_out, not df_train\n",
        "data_modeled, selected_columns = backwardElimination(df_out.values, y, SL, selected_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRjpdwWfpc3L"
      },
      "source": [
        "print('The following columns remain based on p-value selection:')\n",
        "print(selected_columns)\n",
        "# Make a reduced df\n",
        "data_red = pd.DataFrame(data = data_modeled, columns = selected_columns)\n",
        "print(f'After selection by Pearson Correlation, we had {df_out.shape[1]} features.')\n",
        "print(f'After selection by p-value, we have {data_red.shape[1]} features.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8RUeRBHI1ai"
      },
      "source": [
        "### Visualize the distributions of the selected features\n",
        "\n",
        "Ignore the deprecation warnings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6XzPfPcpc3O"
      },
      "source": [
        "# Visualize the selected features\n",
        "fig = plt.figure(figsize = (20, 25))\n",
        "j = 0\n",
        "for i in data_red.columns:\n",
        "    plt.subplot(7, 4, j+1)\n",
        "    j += 1\n",
        "    sns.distplot(data_red[i][y==0], color='g', label = 'no')\n",
        "    sns.distplot(data_red[i][y==1], color='r', label = 'yes')\n",
        "    plt.legend(loc='best')\n",
        "fig.suptitle('Subscription Feature Analysis')\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(top=0.95)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUESDMGAJTOD"
      },
      "source": [
        "### Exercise: Cross-validated LASSO feature importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZCAm2cny8IU"
      },
      "source": [
        "### START CODE HERE ###\n",
        "# Instantiate the cross-validated LASSO regressor\n",
        "reg = None\n",
        "# Train the regressor on the reduced df\n",
        "None\n",
        "### END CODE HERE ###\n",
        "print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\n",
        "print(\"Best score using built-in LassoCV: %f\" %reg.score(data_red, y))\n",
        "coef = pd.Series(reg.coef_, index = data_red.columns)\n",
        "print(f\"Lasso picked {sum(coef != 0)} features and eliminated the other {sum(coef == 0)} features\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-nSBxmEKPh5"
      },
      "source": [
        "### Visualize the LASSO feature importances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h80oTFv8pc3T"
      },
      "source": [
        "imp_coef = coef.sort_values()\n",
        "import matplotlib\n",
        "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
        "imp_coef.plot(kind = \"barh\")\n",
        "plt.title(\"Feature importance using Lasso Model\")\n",
        "#pdays is the only other feature here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Quusm-TAKgNj"
      },
      "source": [
        "### Print summary statistics for the reduced df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-e5nMiApc3W"
      },
      "source": [
        "data_red.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziw5vUYGLb-_"
      },
      "source": [
        "## Random Forest Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0Q8IPz4MCl4"
      },
      "source": [
        "### First, import some convenience functions and perform a 70-30 train-test split split on the (reduced) training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJyPKyeCpc3Y"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "x_train, x_test, y_train, y_test = train_test_split(data_red.values, y.values, test_size = 0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT1-v-OlLgAu"
      },
      "source": [
        "### Exercise: Default Random Forest Classifier, without balancing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yufO3v0wpc3a"
      },
      "source": [
        "### START CODE HERE ###\n",
        "# Instantiate the classifier\n",
        "# Set the max depth to 10 and the random state to 0\n",
        "clf = None\n",
        "# Train the classifier\n",
        "None\n",
        "# Use the classifier to make predictions from the test features\n",
        "prediction = None\n",
        "# Compute the confusion matrix between the true and predicted test targets\n",
        "cm = None\n",
        "# Compute the model's accuracy by summing the diagonals of the confusion matrix\n",
        "# (i.e. taking its trace) and dividing by the number of test samples\n",
        "accuracy = None\n",
        "### END CODE HERE ###\n",
        "print(f\"Accuracy = {accuracy}\")\n",
        "recall_scores = cross_val_score(clf, x_test, y_test, scoring='recall', cv=5)\n",
        "f1_scores = cross_val_score(clf, x_test, y_test, scoring='f1', cv=5)\n",
        "print(f'Mean Recall = {mean(recall_scores):.3f}')\n",
        "print(f'Mean F1 Score = {mean(f1_scores):.3f}')\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le3pcONnUt7c"
      },
      "source": [
        "### Exercise: Random Forest Classifier with weight balancing\n",
        "\n",
        "The target class is highly imbalanced; recall from earlier that only about 11.7% of loan applicants in this dataset are approved. One way around this is to add weight balancing to the classifier.\n",
        "\n",
        "This exercise is otherwise a repeat of the previous one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DJd_Z9lpc3d"
      },
      "source": [
        "### START CODE HERE ###\n",
        "# Instantiate the classifier\n",
        "# Set the max depth to 10, the random state to 0, and the class_weight to 'balanced'\n",
        "clf = None\n",
        "# Train the classifier\n",
        "None\n",
        "# Use the classifier to make predictions from the test features\n",
        "prediction = None\n",
        "# Compute the confusion matrix between the true and predicted test targets\n",
        "cm = None\n",
        "# Compute the model's accuracy by summing the diagonals of the confusion matrix\n",
        "# (i.e. taking its trace) and dividing by the number of test samples\n",
        "accuracy = None\n",
        "### END CODE HERE ###\n",
        "print(f\"Accuracy = {accuracy}\")\n",
        "recall_scores = cross_val_score(clf, x_test, y_test, scoring='recall', cv=5)\n",
        "f1_scores = cross_val_score(clf, x_test, y_test, scoring='f1', cv=5)\n",
        "print(f'Mean Recall = {mean(recall_scores):.3f}')\n",
        "print(f'Mean F1 Score = {mean(f1_scores):.3f}')\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}